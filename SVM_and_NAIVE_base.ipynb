{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqDoJPUMEpdy"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        " What is a Support Vector Machine (SVM)?**\n",
        "SVM is a supervised machine learning algorithm used for classification and regression tasks. It finds the optimal hyperplane that best separates different classes in a dataset by maximizing the margin between data points.\n",
        "\n",
        " What is the difference between Hard Margin and Soft Margin SVM?**\n",
        "- **Hard Margin SVM**: Used when the data is perfectly linearly separable. It strictly maximizes the margin but does not allow any misclassification.\n",
        "- **Soft Margin SVM**: Used when data is not perfectly separable. It allows some misclassification by introducing a penalty (C parameter) to improve generalization.\n",
        "\n",
        " What is the mathematical intuition behind SVM?**\n",
        "SVM aims to maximize the margin between two classes. It finds the hyperplane defined as:\n",
        "\\[\n",
        "w^T x + b = 0\n",
        "\\]\n",
        "by minimizing the norm \\( ||w||^2 \\) while ensuring correct classification:\n",
        "\\[\n",
        "y_i (w^T x_i + b) \\geq 1\n",
        "\\]\n",
        "\n",
        " What is the role of Lagrange Multipliers in SVM?**\n",
        "Lagrange multipliers help convert the constrained optimization problem of SVM into an unconstrained dual form, making it easier to solve using quadratic programming.\n",
        "\n",
        " What are Support Vectors in SVM?**\n",
        "Support Vectors are the data points that lie closest to the decision boundary. They determine the position and orientation of the optimal hyperplane.\n",
        "\n",
        " What is a Support Vector Classifier (SVC)?**\n",
        "SVC is the classification version of SVM, used to separate data into two or more classes using a decision boundary.\n",
        "\n",
        " What is a Support Vector Regressor (SVR)?**\n",
        "SVR is the regression version of SVM that tries to fit a function within a certain margin while minimizing the error.\n",
        "\n",
        " What is the Kernel Trick in SVM?**\n",
        "The Kernel Trick allows SVM to handle non-linearly separable data by mapping it into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        " Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:**\n",
        "- **Linear Kernel**: Used for linearly separable data.\n",
        "- **Polynomial Kernel**: Captures polynomial relationships but is computationally expensive.\n",
        "- **RBF Kernel**: Most commonly used; maps data into infinite-dimensional space and is effective for complex patterns.\n",
        "\n",
        " What is the effect of the C parameter in SVM?**\n",
        "The **C parameter** controls the trade-off between maximizing the margin and minimizing misclassification. A high C reduces the margin but minimizes misclassification, while a low C increases the margin but allows more errors.\n",
        "\n",
        " What is the role of the Gamma parameter in RBF Kernel SVM?**\n",
        "Gamma determines how far the influence of a single training point reaches. A high gamma makes the model focus on close points, while a low gamma captures broader patterns.\n",
        "\n",
        "\n",
        " What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes' Theorem. It is called \"Naïve\" because it assumes that features are independent of each other, which is often not the case in real-world scenarios.\n",
        "\n",
        " What is Bayes’ Theorem?**\n",
        "Bayes’ Theorem states:\n",
        "\\[\n",
        "P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n",
        "\\]\n",
        "It calculates the probability of an event occurring given prior knowledge of related conditions.\n",
        "\n",
        " Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes:**\n",
        "- **Gaussian Naïve Bayes**: Used for continuous data assuming a normal distribution.\n",
        "- **Multinomial Naïve Bayes**: Used for text classification where features represent frequencies.\n",
        "- **Bernoulli Naïve Bayes**: Used for binary features (0/1) like spam classification.\n",
        "\n",
        " When should you use Gaussian Naïve Bayes over other variants?**\n",
        "Gaussian Naïve Bayes is best suited when the dataset consists of continuous variables that follow a normal distribution.\n",
        "\n",
        " What are the key assumptions made by Naïve Bayes?**\n",
        "- Features are independent of each other (conditional independence assumption).\n",
        "- All features contribute equally to the outcome.\n",
        "- The prior probabilities are correctly estimated.\n",
        "\n",
        " What are the advantages and disadvantages of Naïve Bayes?**\n",
        "**Advantages**:\n",
        "- Fast and efficient.\n",
        "- Works well with high-dimensional data.\n",
        "- Requires a small amount of training data.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Assumes feature independence, which is rarely true.\n",
        "- Struggles with feature dependencies.\n",
        "\n",
        "Why is Naïve Bayes a good choice for text classification?**\n",
        "Naïve Bayes performs well in text classification because words (features) often appear independently, making the independence assumption less problematic.\n",
        "\n",
        " Compare SVM and Naïve Bayes for classification tasks:**\n",
        "- **SVM**: Works well for high-dimensional, complex datasets with clear margins.\n",
        "- **Naïve Bayes**: Works well for probabilistic classification, especially for text and spam filtering.\n",
        "\n",
        "How does Laplace Smoothing help in Naïve Bayes?**\n",
        "Laplace Smoothing (additive smoothing) prevents zero probabilities by adding a small constant (e.g., 1) to all frequency counts, ensuring unseen features do not dominate the prediction.\n",
        "\n",
        "Here are the interview-style questions along with Python-based answers:\n",
        "\n",
        "---\n",
        "\n",
        " Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy**\n",
        "```python\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "```\n",
        "\n",
        "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "---python\n",
        "\n",
        " Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies**\n",
        "```python\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
        "\n",
        "linear_svm = SVC(kernel='linear').fit(X_train, y_train)\n",
        "rbf_svm = SVC(kernel='rbf').fit(X_train, y_train)\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", accuracy_score(y_test, linear_svm.predict(X_test)))\n",
        "print(\"RBF Kernel Accuracy:\", accuracy_score(y_test, rbf_svm.predict(X_test)))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)**\n",
        "```python\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
        "\n",
        "model = SVR(kernel='rbf')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary**\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.1)\n",
        "clf = SVC(kernel='poly', degree=3).fit(X, y)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
        "\n",
        "ax = plt.gca()\n",
        "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
        "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
        "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy**\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=42)\n",
        "\n",
        "model = GaussianNB().fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset**\n",
        "```python\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "news = fetch_20newsgroups(subset='train', categories=['rec.autos', 'sci.space'])\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(news.data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, news.target, test_size=0.2, random_state=42)\n",
        "\n",
        "model = MultinomialNB().fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)**\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1], 'kernel': ['linear', 'rbf']}\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(f'Best parameters: {grid.best_params_}')\n",
        "print(f'Best accuracy: {grid.best_score_}')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy**\n",
        "```python\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "svm_model = SVC(kernel='rbf').fit(X_train, y_train)\n",
        "nb_model = GaussianNB().fit(X_train, y_train)\n",
        "\n",
        "svm_acc = accuracy_score(y_test, svm_model.predict(X_test))\n",
        "nb_acc = accuracy_score(y_test, nb_model.predict(X_test))\n",
        "\n",
        "print(f'SVM Accuracy: {svm_acc:.2f}')\n",
        "print(f'Naïve Bayes Accuracy: {nb_acc:.2f}')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score**\n",
        "```python\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_prob = nb_model.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f'ROC-AUC Score: {roc_auc:.2f}')\n",
        "```\n"
      ]
    }
  ]
}