{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "zqYAW3uS2Enk",
        "outputId": "9f3f0201-50d9-48b0-bc51-4164264a2ddb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nHere are the questions and answers for your interview preparation:\\n\\n---\\n\\n### **1. What does R-squared represent in a regression model?**\\n- **Answer:** R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model. It indicates the goodness of fit, with values closer to 1 implying a better fit.\\n\\n---\\n\\n### **2. What are the assumptions of linear regression?**\\n- **Answer:**  \\n  1. **Linearity:** The relationship between independent and dependent variables is linear.  \\n  2. **Independence:** Residuals are independent of each other.  \\n  3. **Homoscedasticity:** The variance of residuals is constant across all levels of the independent variable.  \\n  4. **Normality:** Residuals are normally distributed.  \\n  5. **No multicollinearity:** Independent variables should not be highly correlated with each other.\\n\\n---\\n\\n### **3. What is the difference between R-squared and Adjusted R-squared?**\\n- **Answer:** R-squared increases as more predictors are added, regardless of their relevance. Adjusted R-squared adjusts for the number of predictors and only increases if the new predictor improves the model fit.\\n\\n---\\n\\n### **4. Why do we use Mean Squared Error (MSE)?**\\n- **Answer:** MSE measures the average squared difference between actual and predicted values. It penalizes larger errors more heavily, making it useful for assessing a model’s accuracy.\\n\\n---\\n\\n### **5. What does an Adjusted R-squared value of 0.85 indicate?**\\n- **Answer:** An Adjusted R-squared value of 0.85 indicates that 85% of the variance in the dependent variable is explained by the predictors, considering the number of predictors and sample size.\\n\\n---\\n\\n### **6. How do we check for normality of residuals in linear regression?**\\n- **Answer:** Normality can be checked using:  \\n  1. **Q-Q plots:** Residuals should align along a diagonal line.  \\n  2. **Shapiro-Wilk test:** A statistical test for normality.  \\n  3. **Histogram:** Residuals should form a bell-shaped curve.\\n\\n---\\n\\n### **7. What is multicollinearity, and how does it impact regression?**\\n- **Answer:** Multicollinearity occurs when independent variables are highly correlated. It can inflate standard errors, making it difficult to determine the significance of individual predictors.\\n\\n---\\n\\n### **8. What is Mean Absolute Error (MAE)?**\\n- **Answer:** MAE is the average of the absolute differences between actual and predicted values. It provides a straightforward measure of prediction error.\\n\\n---\\n\\n### **9. What are the benefits of using an ML pipeline?**\\n- **Answer:**  \\n  1. **Automation:** Simplifies repetitive tasks like preprocessing and model training.  \\n  2. **Consistency:** Ensures the same steps are applied each time.  \\n  3. **Scalability:** Easily handles large datasets and multiple models.  \\n  4. **Reproducibility:** Makes the workflow repeatable.\\n\\n---\\n\\n### **10. Why is RMSE considered more interpretable than MSE?**\\n- **Answer:** RMSE is in the same unit as the dependent variable, making it easier to interpret, whereas MSE is in squared units.\\n\\n---\\n\\n### **11. What is pickling in Python, and how is it useful in ML?**\\n- **Answer:** Pickling is a process of serializing and saving Python objects, like trained ML models, for reuse. It helps save time and computational resources by avoiding retraining.\\n\\n---\\n\\n### **12. What does a high R-squared value mean?**\\n- **Answer:** A high R-squared value indicates that a large proportion of the variance in the dependent variable is explained by the model, suggesting a good fit.\\n\\n---\\n\\n### **13. What happens if linear regression assumptions are violated?**\\n- **Answer:**  \\n  - Violations can lead to biased or inefficient estimates.  \\n  - Predictions may be unreliable, and statistical tests might not hold true.\\n\\n---\\n\\n### **14. How can we address multicollinearity in regression?**\\n- **Answer:**  \\n  1. Remove highly correlated variables.  \\n  2. Use techniques like Principal Component Analysis (PCA).  \\n  3. Use regularization methods such as Ridge or Lasso regression.\\n\\n---\\n\\n### **15. How can feature selection improve model performance in regression analysis?**\\n- **Answer:** Feature selection removes irrelevant or redundant predictors, reducing overfitting, improving interpretability, and increasing computational efficiency.\\n\\n---\\n\\n### **16. How is Adjusted R-squared calculated?**\\n- **Answer:** Adjusted R-squared is calculated using:  \\n  \\\\[\\n  \\text{Adjusted } R^2 = 1 - \\\\left( \\x0crac{(1 - R^2)(n - 1)}{n - k - 1} \\right)\\n  \\\\]  \\n  where \\\\( n \\\\) is the number of observations and \\\\( k \\\\) is the number of predictors.\\n\\n---\\n\\n### **17. Why is MSE sensitive to outliers?**\\n- **Answer:** MSE squares the errors, amplifying the impact of large deviations (outliers) on the overall error calculation.\\n\\n---\\n\\n### **18. What is the role of homoscedasticity in linear regression?**\\n- **Answer:** Homoscedasticity ensures that the variance of residuals is constant, which is essential for reliable coefficient estimates and valid hypothesis testing.\\n\\n---\\n\\n### **19. What is Root Mean Squared Error (RMSE)?**\\n- **Answer:** RMSE is the square root of the MSE, representing the standard deviation of residuals. It measures the model’s prediction accuracy in the same units as the dependent variable.\\n\\n---\\n\\n### **20. Why is pickling considered risky?**\\n- **Answer:** Pickling is risky because it can execute arbitrary code during deserialization, making it vulnerable to malicious code injection.\\n\\n---\\n\\n### **21. What alternatives exist to pickling for saving ML models?**\\n- **Answer:**  \\n  1. **Joblib:** Efficient for large arrays.  \\n  2. **ONNX:** Open format for ML models.  \\n  3. **HDF5:** Stores large datasets.  \\n  4. **PMML:** XML-based model format.\\n\\n---\\n\\n### **22. What is heteroscedasticity, and why is it a problem?**\\n- **Answer:** Heteroscedasticity occurs when residual variance changes across levels of an independent variable. It violates regression assumptions, leading to inefficient estimates and unreliable hypothesis testing.\\n\\n---\\n\\n### **23. How can interaction terms enhance a regression model's predictive power?**\\n- **Answer:** Interaction terms capture the combined effect of two or more predictors, revealing relationships that are not evident from individual effects.\\n\\n--- \\n\\nLet me know if you'd like any of these topics explained in greater detail!\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "\n",
        " 1. What does R-squared represent in a regression model?**\n",
        "- **Answer:** R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model. It indicates the goodness of fit, with values closer to 1 implying a better fit.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. What are the assumptions of linear regression?**\n",
        "- **Answer:**\n",
        "  1. **Linearity:** The relationship between independent and dependent variables is linear.\n",
        "  2. **Independence:** Residuals are independent of each other.\n",
        "  3. **Homoscedasticity:** The variance of residuals is constant across all levels of the independent variable.\n",
        "  4. **Normality:** Residuals are normally distributed.\n",
        "  5. **No multicollinearity:** Independent variables should not be highly correlated with each other.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. What is the difference between R-squared and Adjusted R-squared?**\n",
        "- **Answer:** R-squared increases as more predictors are added, regardless of their relevance. Adjusted R-squared adjusts for the number of predictors and only increases if the new predictor improves the model fit.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Why do we use Mean Squared Error (MSE)?**\n",
        "- **Answer:** MSE measures the average squared difference between actual and predicted values. It penalizes larger errors more heavily, making it useful for assessing a model’s accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. What does an Adjusted R-squared value of 0.85 indicate?**\n",
        "- **Answer:** An Adjusted R-squared value of 0.85 indicates that 85% of the variance in the dependent variable is explained by the predictors, considering the number of predictors and sample size.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. How do we check for normality of residuals in linear regression?**\n",
        "- **Answer:** Normality can be checked using:\n",
        "  1. **Q-Q plots:** Residuals should align along a diagonal line.\n",
        "  2. **Shapiro-Wilk test:** A statistical test for normality.\n",
        "  3. **Histogram:** Residuals should form a bell-shaped curve.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. What is multicollinearity, and how does it impact regression?**\n",
        "- **Answer:** Multicollinearity occurs when independent variables are highly correlated. It can inflate standard errors, making it difficult to determine the significance of individual predictors.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. What is Mean Absolute Error (MAE)?**\n",
        "- **Answer:** MAE is the average of the absolute differences between actual and predicted values. It provides a straightforward measure of prediction error.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. What are the benefits of using an ML pipeline?**\n",
        "- **Answer:**\n",
        "  1. **Automation:** Simplifies repetitive tasks like preprocessing and model training.\n",
        "  2. **Consistency:** Ensures the same steps are applied each time.\n",
        "  3. **Scalability:** Easily handles large datasets and multiple models.\n",
        "  4. **Reproducibility:** Makes the workflow repeatable.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Why is RMSE considered more interpretable than MSE?**\n",
        "- **Answer:** RMSE is in the same unit as the dependent variable, making it easier to interpret, whereas MSE is in squared units.\n",
        "\n",
        "---\n",
        "\n",
        "### **11. What is pickling in Python, and how is it useful in ML?**\n",
        "- **Answer:** Pickling is a process of serializing and saving Python objects, like trained ML models, for reuse. It helps save time and computational resources by avoiding retraining.\n",
        "\n",
        "---\n",
        "\n",
        "### **12. What does a high R-squared value mean?**\n",
        "- **Answer:** A high R-squared value indicates that a large proportion of the variance in the dependent variable is explained by the model, suggesting a good fit.\n",
        "\n",
        "---\n",
        "\n",
        "### **13. What happens if linear regression assumptions are violated?**\n",
        "- **Answer:**\n",
        "  - Violations can lead to biased or inefficient estimates.\n",
        "  - Predictions may be unreliable, and statistical tests might not hold true.\n",
        "\n",
        "---\n",
        "\n",
        "### **14. How can we address multicollinearity in regression?**\n",
        "- **Answer:**\n",
        "  1. Remove highly correlated variables.\n",
        "  2. Use techniques like Principal Component Analysis (PCA).\n",
        "  3. Use regularization methods such as Ridge or Lasso regression.\n",
        "\n",
        "---\n",
        "\n",
        "### **15. How can feature selection improve model performance in regression analysis?**\n",
        "- **Answer:** Feature selection removes irrelevant or redundant predictors, reducing overfitting, improving interpretability, and increasing computational efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "### **16. How is Adjusted R-squared calculated?**\n",
        "- **Answer:** Adjusted R-squared is calculated using:\n",
        "  \\[\n",
        "  \\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right)\n",
        "  \\]\n",
        "  where \\( n \\) is the number of observations and \\( k \\) is the number of predictors.\n",
        "\n",
        "---\n",
        "\n",
        "### **17. Why is MSE sensitive to outliers?**\n",
        "- **Answer:** MSE squares the errors, amplifying the impact of large deviations (outliers) on the overall error calculation.\n",
        "\n",
        "---\n",
        "\n",
        "### **18. What is the role of homoscedasticity in linear regression?**\n",
        "- **Answer:** Homoscedasticity ensures that the variance of residuals is constant, which is essential for reliable coefficient estimates and valid hypothesis testing.\n",
        "\n",
        "---\n",
        "\n",
        "### **19. What is Root Mean Squared Error (RMSE)?**\n",
        "- **Answer:** RMSE is the square root of the MSE, representing the standard deviation of residuals. It measures the model’s prediction accuracy in the same units as the dependent variable.\n",
        "\n",
        "---\n",
        "\n",
        "### **20. Why is pickling considered risky?**\n",
        "- **Answer:** Pickling is risky because it can execute arbitrary code during deserialization, making it vulnerable to malicious code injection.\n",
        "\n",
        "---\n",
        "\n",
        "### **21. What alternatives exist to pickling for saving ML models?**\n",
        "- **Answer:**\n",
        "  1. **Joblib:** Efficient for large arrays.\n",
        "  2. **ONNX:** Open format for ML models.\n",
        "  3. **HDF5:** Stores large datasets.\n",
        "  4. **PMML:** XML-based model format.\n",
        "\n",
        "---\n",
        "\n",
        "### **22. What is heteroscedasticity, and why is it a problem?**\n",
        "- **Answer:** Heteroscedasticity occurs when residual variance changes across levels of an independent variable. It violates regression assumptions, leading to inefficient estimates and unreliable hypothesis testing.\n",
        "\n",
        "---\n",
        "\n",
        "### **23. How can interaction terms enhance a regression model's predictive power?**\n",
        "- **Answer:** Interaction terms capture the combined effect of two or more predictors, revealing relationships that are not evident from individual effects.\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you'd like any of these topics explained in greater detail!\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "### **1. Visualize the distribution of residuals (Seaborn's diamonds dataset):**\n",
        "```python\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Prepare data\n",
        "X = diamonds[['carat', 'depth', 'table']]  # Select some features\n",
        "y = diamonds['price']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit linear regression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# Plot residuals distribution\n",
        "sns.histplot(residuals, kde=True, bins=30)\n",
        "plt.title(\"Distribution of Residuals\")\n",
        "plt.xlabel(\"Residuals\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Calculate MSE, MAE, and RMSE for a linear regression model:**\n",
        "```python\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Check linear regression assumptions:**\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Linearity check\n",
        "sns.pairplot(diamonds[['carat', 'price']])\n",
        "plt.title(\"Linearity Check\")\n",
        "plt.show()\n",
        "\n",
        "# Residuals plot for homoscedasticity\n",
        "sns.scatterplot(x=y_pred, y=residuals)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title(\"Residuals vs Fitted\")\n",
        "plt.xlabel(\"Fitted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix for multicollinearity\n",
        "corr_matrix = diamonds.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **4. ML pipeline with feature scaling and regression models:**\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', RandomForestRegressor())\n",
        "])\n",
        "\n",
        "# Fit model and evaluate\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(f\"R-squared score: {r2_score(y_test, y_pred)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Simple linear regression with coefficients and R-squared:**\n",
        "```python\n",
        "# Print coefficients\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared: {model.score(X_test, y_test)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Analyze total bill and tip relationship (Seaborn's tips dataset):**\n",
        "```python\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Linear regression\n",
        "X = tips[['total_bill']]\n",
        "y = tips['tip']\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Plot\n",
        "sns.regplot(x='total_bill', y='tip', data=tips, line_kws={\"color\": \"red\"})\n",
        "plt.title(\"Total Bill vs Tip\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Fit linear regression on synthetic data:**\n",
        "```python\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate data\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, model.predict(X), color='red')\n",
        "plt.title(\"Regression Line\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Pickle a linear regression model:**\n",
        "```python\n",
        "import pickle\n",
        "\n",
        "# Save model\n",
        "with open('linear_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Load model\n",
        "with open('linear_model.pkl', 'rb') as f:\n",
        "    loaded_model = pickle.load(f)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Polynomial regression (degree 2):**\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Polynomial features\n",
        "poly_model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
        "poly_model.fit(X, y)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, poly_model.predict(X), color='red')\n",
        "plt.title(\"Polynomial Regression (Degree 2)\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Generate synthetic data for linear regression:**\n",
        "```python\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=20)\n",
        "model.fit(X, y)\n",
        "print(f\"Coefficient: {model.coef_}, Intercept: {model.intercept_}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **11. Compare polynomial regression models:**\n",
        "```python\n",
        "for degree in range(1, 5):\n",
        "    poly_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "    poly_model.fit(X, y)\n",
        "    print(f\"Degree {degree}: R-squared = {poly_model.score(X, y)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **12. Fit simple linear regression with two features:**\n",
        "```python\n",
        "X = diamonds[['carat', 'depth']]\n",
        "y = diamonds['price']\n",
        "model.fit(X, y)\n",
        "print(f\"Coefficients: {model.coef_}, Intercept: {model.intercept_}, R-squared: {model.score(X, y)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you want detailed scripts for the remaining!\n",
        "'''"
      ],
      "metadata": {
        "id": "gv2VkQjT3Lbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}