{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx455L_-39JK"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "1. **What is Logistic Regression, and how does it differ from Linear Regression?**\n",
        "   **Answer:** Logistic Regression is a classification algorithm used to predict categorical outcomes (e.g., binary classification). Unlike Linear Regression, which predicts continuous values, Logistic Regression applies the sigmoid function to restrict outputs between 0 and 1, representing probabilities.\n",
        "\n",
        "2. **What is the mathematical equation of Logistic Regression?**\n",
        "   **Answer:** The equation is:\n",
        "   \\[\n",
        "   P(Y=1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n)}}\n",
        "   \\]\n",
        "   where \\( P(Y=1 | X) \\) is the probability of the positive class.\n",
        "\n",
        "3. **Why do we use the Sigmoid function in Logistic Regression?**\n",
        "   **Answer:** The sigmoid function maps any real number into the range (0,1), allowing us to interpret outputs as probabilities.\n",
        "\n",
        "4. **What is the cost function of Logistic Regression?**\n",
        "   **Answer:** Logistic Regression uses **log loss (binary cross-entropy)** as its cost function:\n",
        "   \\[\n",
        "   J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right]\n",
        "   \\]\n",
        "   where \\( h_\\theta(x) \\) is the predicted probability.\n",
        "5. **What is Regularization in Logistic Regression? Why is it needed?**\n",
        "   **Answer:** Regularization prevents overfitting by adding a penalty to the cost function. It helps keep model weights small, improving generalization.\n",
        "\n",
        "6. **Explain the difference between Lasso, Ridge, and Elastic Net regression.**\n",
        "   **Answer:**\n",
        "   - **Ridge Regression (L2):** Adds \\( \\lambda \\sum \\theta^2 \\), shrinking coefficients but keeping all features.\n",
        "   - **Lasso Regression (L1):** Adds \\( \\lambda \\sum |\\theta| \\), leading to feature selection by setting some coefficients to zero.\n",
        "   - **Elastic Net:** Combines L1 and L2 to balance regularization and feature selection.\n",
        "\n",
        "7. **When should we use Elastic Net instead of Lasso or Ridge?**\n",
        "   **Answer:** Elastic Net is used when there are **highly correlated features**, as it retains group features while performing feature selection.\n",
        "\n",
        "8. **What is the impact of the regularization parameter (λ) in Logistic Regression?**\n",
        "   **Answer:**\n",
        "   - **High λ:** More regularization, reducing overfitting but increasing bias.\n",
        "   - **Low λ:** Less regularization, capturing more details but increasing overfitting.\n",
        "9. **What are the key assumptions of Logistic Regression?**\n",
        "   **Answer:**\n",
        "   - The relationship between independent variables and log-odds is linear.\n",
        "   - No multicollinearity among independent variables.\n",
        "   - No extreme outliers.\n",
        "   - Independent observations.\n",
        "\n",
        "10. **What are some alternatives to Logistic Regression for classification tasks?**\n",
        "   **Answer:**\n",
        "   - Decision Trees\n",
        "   - Random Forest\n",
        "   - Support Vector Machines (SVM)\n",
        "   - Neural Networks\n",
        "   - Naïve Bayes\n",
        "\n",
        "11. **What are Classification Evaluation Metrics?**\n",
        "   **Answer:**\n",
        "   - Accuracy\n",
        "   - Precision, Recall, and F1-score\n",
        "   - ROC-AUC Score\n",
        "   - Log Loss\n",
        "\n",
        "12. **How does class imbalance affect Logistic Regression?**\n",
        "   **Answer:** Class imbalance leads to biased predictions favoring the majority class. Techniques like **oversampling, undersampling, and weighted loss functions** can help.\n",
        "\n",
        "13. **What is Hyperparameter Tuning in Logistic Regression?**\n",
        "   **Answer:** It involves finding the best values for hyperparameters like **regularization strength (λ) and solver type** using techniques like Grid Search and Random Search.\n",
        "\n",
        "14. **What are different solvers in Logistic Regression? Which one should be used?**\n",
        "   **Answer:**\n",
        "   - **lbfgs** (default, best for small-medium datasets)\n",
        "   - **liblinear** (good for small datasets, supports L1/L2)\n",
        "   - **saga** (best for large datasets, supports L1/L2/Elastic Net)\n",
        "\n",
        "15. **How is Logistic Regression extended for multiclass classification?**\n",
        "   **Answer:**\n",
        "   - **One-vs-Rest (OvR):** Trains a separate model for each class against the rest.\n",
        "   - **Softmax Regression:** Extends Logistic Regression using the **softmax function** for multi-class probability distribution.\n",
        "\n",
        "16. **What are the advantages and disadvantages of Logistic Regression?**\n",
        "   **Answer:**\n",
        "   **Advantages:**\n",
        "   - Simple and interpretable\n",
        "   - Works well for linearly separable data\n",
        "   - Computationally efficient\n",
        "   **Disadvantages:**\n",
        "   - Struggles with non-linear relationships\n",
        "   - Assumes no multicollinearity\n",
        "\n",
        "17. **What are some use cases of Logistic Regression?**\n",
        "   **Answer:**\n",
        "   - Spam detection\n",
        "   - Customer churn prediction\n",
        "   - Disease diagnosis (e.g., diabetes prediction)\n",
        "   - Fraud detection\n",
        "\n",
        "\n",
        "18. **What is the difference between Softmax Regression and Logistic Regression?**\n",
        "   **Answer:** Softmax Regression generalizes Logistic Regression for **multi-class classification** by computing probabilities for multiple classes.\n",
        "\n",
        "19. **How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
        "   **Answer:**\n",
        "   - **OvR:** Faster, works well for imbalanced data.\n",
        "   - **Softmax:** Provides direct multi-class probabilities, preferred for balanced datasets.\n",
        "\n",
        "20. **How do we interpret coefficients in Logistic Regression?**\n",
        "   **Answer:** Each coefficient \\( \\beta_i \\) represents the **log-odds change** for a one-unit increase in \\( X_i \\), holding other variables constant. The odds ratio is given by \\( e^{\\beta_i} \\).\n",
        "'''\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, f1_score, matthews_corrcoef, cohen_kappa_score\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "\n",
        "# L1 Regularization (Lasso)\n",
        "l1_model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "l1_model.fit(X_train, y_train)\n",
        "l1_pred = l1_model.predict(X_test)\n",
        "print(f'L1 Regularization Accuracy: {accuracy_score(y_test, l1_pred)}')\n",
        "\n",
        "# L2 Regularization (Ridge)\n",
        "l2_model = LogisticRegression(penalty='l2', max_iter=1000)\n",
        "l2_model.fit(X_train, y_train)\n",
        "l2_pred = l2_model.predict(X_test)\n",
        "print(f'L2 Regularization Accuracy: {accuracy_score(y_test, l2_pred)}')\n",
        "print(f'Coefficients: {l2_model.coef_}')\n",
        "\n",
        "# Elastic Net Regularization\n",
        "elastic_model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
        "elastic_model.fit(X_train, y_train)\n",
        "elastic_pred = elastic_model.predict(X_test)\n",
        "print(f'Elastic Net Accuracy: {accuracy_score(y_test, elastic_pred)}')\n",
        "\n",
        "# Multiclass Classification (OvR)\n",
        "ovr_model = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "ovr_model.fit(X_train, y_train)\n",
        "ovr_pred = ovr_model.predict(X_test)\n",
        "print(f'Multiclass OvR Accuracy: {accuracy_score(y_test, ovr_pred)}')\n",
        "\n",
        "# Hyperparameter Tuning with GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
        "grid_search = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=1000), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Accuracy: {grid_search.best_score_}')\n",
        "\n",
        "# Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "skf_scores = cross_val_score(model, X, y, cv=skf)\n",
        "print(f'Average Accuracy (Stratified K-Fold): {skf_scores.mean()}')\n",
        "\n",
        "# Load dataset from CSV (Example usage, replace 'data.csv' with actual file)\n",
        "# df = pd.read_csv('data.csv')\n",
        "# X = df.drop(columns=['target'])\n",
        "# y = df['target']\n",
        "\n",
        "# RandomizedSearchCV for Hyperparameter Tuning\n",
        "param_dist = {'C': np.logspace(-3, 3, 10), 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga']}\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=1000), param_dist, cv=5, n_iter=10, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(f'Best Randomized Parameters: {random_search.best_params_}')\n",
        "\n",
        "# One-vs-One Multiclass Logistic Regression\n",
        "ovo_model = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "ovo_model.fit(X_train, y_train)\n",
        "ovo_pred = ovo_model.predict(X_test)\n",
        "print(f'One-vs-One Accuracy: {accuracy_score(y_test, ovo_pred)}')\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Precision, Recall, and F1-Score\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "print(f'MCC Score: {mcc_score}')\n",
        "\n",
        "# Cohen’s Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f'Cohen’s Kappa Score: {kappa_score}')\n",
        "\n",
        "# Save and Load Model\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "loaded_pred = loaded_model.predict(X_test)\n",
        "print(f'Loaded Model Accuracy: {accuracy_score(y_test, loaded_pred)}')\n",
        "'''"
      ]
    }
  ]
}